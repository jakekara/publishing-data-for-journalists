# Publishing data with journalists in mind

**Notes for 2017-01-27 at Hartford Foundation for Public Giving Data Meetup**

#### You've sent me a press release

* *A+* - In addition to the release, you send all of your raw data, so I
can check your work. Because you did that, I start asking a lot of other
questions and I end up writing several stories on this topic. Plus, you
send it embargoed for a couple weeks, so I have more time to write a better
story. I might even find a mistake, and since it's embargoed, you can fix
it before it gets published.

* **C** -  You send a spreadsheet, but it only has the results of your
computations, not the raw data. I can't do anything with that.

But that's the short game.

#### ... Years later...

The data you collected are public, so chances are people will keep finding
them when trying to answer other questions you never imagined. I'm usually
that person, and my process will stress test your data.

You  want any data you put out there to be "good", meaning:

1. The  methodology (collection plus any analysis or transformation) is
well-documented; 
2. Any caveats about the data are as clear as possible.

You want it to be good, and public, because:

1. You spent the time to collect it, and your time is valuable;
2. Its part of your and your organization's reputation, just like anything
else you put out in the public domain.

If it's not good:

1. It won't be reliable, but people might not know;
2. People might use it in a way you didn't to write a news story or make a
policy decision;
3. People might realize it was unreliable and be :( at you.

Some bad things that I've come across:

#### 1. Data are incomplete in a non-obvious way

Looking into some state data on small business loans, I found something
interesting: A lot of them were past due for an audit to see if they were
meeting their job-creation requirements. When I called to ask about the
data, I was told there was a lot more!

It turns out, the state agency only publishes data on loans it oversees,
while private lenders oversee a bunch of the loans, and the state had much
less information about those loans.

**Problem**: I had to wait for the new data and redo my work. Boo hoo.

**Bigger problem**: I might not have called and come to a wrong
  conclusion. That's especially likely if nothing about the data surprised
  me, or if I was working on a story about a different topic and the
  information I needed from this data set was very basic, like whether a
  specific company had received a loan through this program.
  
**Solution**: Make it as clear as possible in the meta-data what is and
  isn't included in this data. Even give examples of what conclusions you
  can and can't make from it. For instance, the FBI warns not to compare
  its uniform crime data from one year to the next, because agency
  participation in the data collection is so inconsistent.

#### 2. The methodology seems right, but...

The CDC reported that Connecticut had a crazy high decline in teen birth
rates in rural areas: 73 percent over five years. But...

They based this on county-level data. Connecticut is tiny and has eight
counties. When I dug to find which counties they considered rural, it
turned out that Litchfield and Windham counties were classified as rural at
the start of their data, but Windham, which had a higher birth rate, was
reclassified in one of the years, and that threw the data off.

**The good news**: Because their methodology was well documented, this
  quirk in the data was discoverable.

#### 3. This isn't normal

A spreadsheet that contains a row for each record, but then also contains